{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T04:12:09.958617Z",
     "start_time": "2024-11-09T04:12:09.952560Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "id": "c7b26006be838c20",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## `pandas.Series`\n",
    "\n",
    "```python\n",
    "class pandas.Series(\n",
    "    data=None, index=None, dtype=None, name=None, copy=None, fastpath=<no_default>\n",
    ")[source]\n",
    "```\n",
    "\n",
    "One-dimensional `ndarray` with axis labels (including time series).\n",
    "\n",
    "Labels need not be unique but must be a hashable type. The object supports both integer- and label-based indexing and provides a host of methods for performing operations involving the index. Statistical methods from `ndarray` have been overridden to automatically exclude missing data (currently represented as `NaN`).\n",
    "\n",
    "Operations between Series (`+`, `-`, `/`, `*`, `**`) align values based on their associated index values—they need not be the same length. The result index will be the sorted union of the two indexes.\n",
    "\n",
    "### Parameters:\n",
    "\n",
    "- **data**: `array-like`, `Iterable`, `dict`, or scalar value  \n",
    "  Contains data stored in Series. If `data` is a `dict`, argument order is maintained.\n",
    "\n",
    "- **index**: `array-like` or `Index` (1d)  \n",
    "  Values must be hashable and have the same length as `data`. Non-unique index values are allowed. Will default to `RangeIndex` (0, 1, 2, …, n) if not provided. If `data` is dict-like and `index` is None, then the keys in the `data` are used as the `index`. If the `index` is not None, the resulting Series is reindexed with the index values.\n",
    "\n",
    "- **dtype**: `str`, `numpy.dtype`, or `ExtensionDtype`, optional  \n",
    "  Data type for the output Series. If not specified, this will be inferred from `data`. See the user guide for more usages.\n",
    "\n",
    "- **name**: `Hashable`, default `None`  \n",
    "  The name to give to the Series.\n",
    "\n",
    "- **copy**: `bool`, default `False`  \n",
    "  Copy input data. Only affects `Series` or 1d ndarray input. See examples."
   ],
   "id": "4fb0c85e55be4e98"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T04:13:10.492356Z",
     "start_time": "2024-11-09T04:13:10.480332Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# From an array\n",
    "data_array = [1, 2, 3, 4]\n",
    "s1 = pd.Series(data_array)\n",
    "print(s1)\n",
    "\n",
    "# From a dictionary\n",
    "data_dict = {'a': 1, 'b': 2, 'c': 3}\n",
    "s2 = pd.Series(data_dict)\n",
    "print(s2)\n"
   ],
   "id": "d491112bff3d06cc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    2\n",
      "2    3\n",
      "3    4\n",
      "dtype: int64\n",
      "a    1\n",
      "b    2\n",
      "c    3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T04:13:19.812736Z",
     "start_time": "2024-11-09T04:13:19.806858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# From an array with custom index\n",
    "s3 = pd.Series(data_array, index=['A', 'B', 'C', 'D'])\n",
    "print(s3)\n"
   ],
   "id": "95deff11ac6fe64b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A    1\n",
      "B    2\n",
      "C    3\n",
      "D    4\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T04:13:23.832472Z",
     "start_time": "2024-11-09T04:13:23.826608Z"
    }
   },
   "cell_type": "code",
   "source": [
    "s4 = pd.Series(data_array, dtype='float64', name='sample_series')\n",
    "print(s4)\n"
   ],
   "id": "bb052d720f3cb870",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1.0\n",
      "1    2.0\n",
      "2    3.0\n",
      "3    4.0\n",
      "Name: sample_series, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**pandas.DataFrame class**\n",
    "\n",
    "`pandas.DataFrame(data=None, index=None, columns=None, dtype=None, copy=None)[source]`\n",
    "\n",
    "Two-dimensional, size-mutable, potentially heterogeneous tabular data. Data structure also contains labeled axes (rows and columns). Arithmetic operations align on both row and column labels. Can be thought of as a dict-like container for Series objects. The primary pandas data structure.\n",
    "\n",
    "**Parameters:**\n",
    "\n",
    "- **data**: ndarray (structured or homogeneous), Iterable, dict, or DataFrame. Dict can contain Series, arrays, constants, dataclass or list-like objects. If data is a dict, column order follows insertion-order. If a dict contains Series which have an index defined, it is aligned by its index. This alignment also occurs if data is a Series or a DataFrame itself. Alignment is done on Series/DataFrame inputs. If data is a list of dicts, column order follows insertion-order.\n",
    "- **index**: Index or array-like. Index to use for resulting frame. Will default to RangeIndex if no indexing information part of input data and no index provided.\n",
    "- **columns**: Index or array-like. Column labels to use for resulting frame when data does not have them, defaulting to RangeIndex(0, 1, 2, …, n). If data contains column labels, will perform column selection instead.\n",
    "- **dtype**: dtype, default None. Data type to force. Only a single dtype is allowed. If None, infer.\n",
    "- **copy**: bool or None, default None. Copy data from inputs. For dict data, the default of None behaves like copy=True. For DataFrame or 2d ndarray input, the default of None behaves like copy=False. If data is a dict containing one or more Series (possibly of different dtypes), copy=False will ensure that these inputs are not copied."
   ],
   "id": "d17e0ff74cc60187"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T04:01:28.178746Z",
     "start_time": "2024-11-09T04:01:27.591471Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "3be11a3d72e5da2f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-09T04:05:24.815072Z",
     "start_time": "2024-11-09T04:05:24.809120Z"
    }
   },
   "source": [
    "d={'col1':[1,2,3],'col2':[4,5,6]}\n",
    "pd.DataFrame(data=d)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   col1  col2\n",
       "0     1     4\n",
       "1     2     5\n",
       "2     3     6"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T04:05:55.296169Z",
     "start_time": "2024-11-09T04:05:55.278306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "f={'col1':[1,2,3,4,5,6,7,7,7],'col2':[4,5,6,7,8,9,10,11,12],'col3':pd.Series([1,2,3],index=[0,1,2])}\n",
    "pd.DataFrame(data=f, index=[1,2,3,4,5,6,7,8,9])"
   ],
   "id": "228680cfd67c567",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   col1  col2  col3\n",
       "1     1     4   2.0\n",
       "2     2     5   3.0\n",
       "3     3     6   NaN\n",
       "4     4     7   NaN\n",
       "5     5     8   NaN\n",
       "6     6     9   NaN\n",
       "7     7    10   NaN\n",
       "8     7    11   NaN\n",
       "9     7    12   NaN"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "      <th>col3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## `pandas.read_csv`\n",
    "\n",
    "```python\n",
    "pandas.read_csv(\n",
    "    filepath_or_buffer,\n",
    "    *,\n",
    "    sep=<no_default>,\n",
    "    delimiter=None,\n",
    "    header='infer',\n",
    "    names=<no_default>,\n",
    "    index_col=None,\n",
    "    usecols=None,\n",
    "    dtype=None,\n",
    "    engine=None,\n",
    "    converters=None,\n",
    "    true_values=None,\n",
    "    false_values=None,\n",
    "    skipinitialspace=False,\n",
    "    skiprows=None,\n",
    "    skipfooter=0,\n",
    "    nrows=None,\n",
    "    na_values=None,\n",
    "    keep_default_na=True,\n",
    "    na_filter=True,\n",
    "    verbose=<no_default>,\n",
    "    skip_blank_lines=True,\n",
    "    parse_dates=None,\n",
    "    infer_datetime_format=<no_default>,\n",
    "    keep_date_col=<no_default>,\n",
    "    date_parser=<no_default>,\n",
    "    date_format=None,\n",
    "    dayfirst=False,\n",
    "    cache_dates=True,\n",
    "    iterator=False,\n",
    "    chunksize=None,\n",
    "    compression='infer',\n",
    "    thousands=None,\n",
    "    decimal='.',\n",
    "    lineterminator=None,\n",
    "    quotechar='\"',\n",
    "    quoting=0,\n",
    "    doublequote=True,\n",
    "    escapechar=None,\n",
    "    comment=None,\n",
    "    encoding=None,\n",
    "    encoding_errors='strict',\n",
    "    dialect=None,\n",
    "    on_bad_lines='error',\n",
    "    delim_whitespace=<no_default>,\n",
    "    low_memory=True,\n",
    "    memory_map=False,\n",
    "    float_precision=None,\n",
    "    storage_options=None,\n",
    "    dtype_backend=<no_default>\n",
    ")[source]\n",
    "```\n",
    "\n",
    "Read a comma-separated values (CSV) file into a DataFrame.\n",
    "\n",
    "Also supports optionally iterating or breaking the file into chunks.\n",
    "\n",
    "Additional help can be found in the online docs for IO Tools.\n",
    "\n",
    "### Parameters:\n",
    "\n",
    "- **filepath_or_buffer**: `str`, path object or file-like object  \n",
    "  Any valid string path is acceptable. The string could be a URL. Valid URL schemes include `http`, `ftp`, `s3`, `gs`, and `file`. For file URLs, a host is expected. A local file could be: `file://localhost/path/to/table.csv`.\n",
    "\n",
    "  If you want to pass in a path object, pandas accepts any `os.PathLike`.\n",
    "\n",
    "  By file-like object, we refer to objects with a read() method, such as a file handle (e.g., via built-in open function) or `StringIO`.\n",
    "\n",
    "- **sep**: `str`, default `‘,’`  \n",
    "  Character or regex pattern to treat as the delimiter. If `sep=None`, the C engine cannot automatically detect the separator, but the Python parsing engine can, meaning the latter will be used and automatically detect the separator from only the first valid row of the file by Python’s built-in sniffer tool, `csv.Sniffer`. In addition, separators longer than 1 character and different from '\\s+' will be interpreted as regular expressions and will also force the use of the Python parsing engine. Note that regex delimiters are prone to ignoring quoted data. Regex example: `'\\r\\t'`.\n",
    "\n",
    "- **delimiter**: `str`, optional  \n",
    "  Alias for `sep`.\n",
    "\n",
    "- **header**: `int`, `Sequence of int`, `‘infer’` or `None`, default `‘infer’`  \n",
    "  Row number(s) containing column labels and marking the start of the data (zero-indexed). Default behavior is to infer the column names: if no names are passed, the behavior is identical to `header=0` and column names are inferred from the first line of the file. If column names are passed explicitly to `names`, then the behavior is identical to `header=None`. Explicitly pass `header=0` to be able to replace existing names. The header can be a list of integers that specify row locations for a MultiIndex on the columns, e.g., `[0, 1, 3]`. Intervening rows that are not specified will be skipped (e.g., 2 in this example is skipped). Note that this parameter ignores commented lines and empty lines if `skip_blank_lines=True`, so `header=0` denotes the first line of data rather than the first line of the file.\n",
    "\n",
    "- **names**: `Sequence of Hashable`, optional  \n",
    "  Sequence of column labels to apply. If the file contains a header row, then you should explicitly pass `header=0` to override the column names. Duplicates in this list are not allowed.\n",
    "\n",
    "- **index_col**: `Hashable`, `Sequence of Hashable` or `False`, optional  \n",
    "  Column(s) to use as row label(s), denoted either by column labels or column indices. If a sequence of labels or indices is given, `MultiIndex` will be formed for the row labels.\n",
    "\n",
    "  Note: `index_col=False` can be used to force `pandas` to not use the first column as the index, e.g., when you have a malformed file with delimiters at the end of each line.\n",
    "\n",
    "- **usecols**: `Sequence of Hashable` or `Callable`, optional  \n",
    "  Subset of columns to select, denoted either by column labels or column indices. If list-like, all elements must either be positional (i.e., integer indices into the document columns) or strings that correspond to column names provided either by the user in `names` or inferred from the document header row(s). If `names` are given, the document header row(s) are not taken into account. For example, a valid list-like `usecols` parameter would be `[0, 1, 2]` or `['foo', 'bar', 'baz']`. Element order is ignored, so `usecols=[0, 1]` is the same as `[1, 0]`. To instantiate a DataFrame from data with element order preserved use `pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']]` for columns in `['foo', 'bar']` order or `pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']]` for `['bar', 'foo']` order.\n",
    "\n",
    "  If callable, the callable function will be evaluated against the column names, returning names where the callable function evaluates to True. An example of a valid callable argument would be `lambda x: x.upper() in ['AAA', 'BBB', 'DDD']`. Using this parameter results in much faster parsing time and lower memory usage.\n",
    "\n",
    "- **dtype**: `dtype` or `dict of {Hashable: dtype}`, optional  \n",
    "  Data type(s) to apply to either the whole dataset or individual columns, e.g., `{'a': np.float64, 'b': np.int32, 'c': 'Int64'}`. Use `str` or `object` together with suitable `na_values` settings to preserve and not interpret dtype. If converters are specified, they will be applied instead of dtype conversion.\n",
    "\n",
    "  Added in version 1.5.0: Support for `defaultdict` was added. Specify a `defaultdict` as input where the default determines the dtype of the columns that are not explicitly listed.\n",
    "\n",
    "- **engine**: {‘c’, ‘python’, ‘pyarrow’}, optional  \n",
    "  Parser engine to use. The `C` and `pyarrow` engines are faster, while the `python` engine is currently more feature-complete. Multithreading is currently only supported by the `pyarrow` engine.\n",
    "\n",
    "  Added in version 1.4.0: The `‘pyarrow’` engine was added as an experimental engine, and some features are unsupported or may not work correctly with this engine.\n",
    "\n",
    "- **converters**: `dict of {Hashable: Callable}`, optional  \n",
    "  Functions for converting values in specified columns. Keys can either be column labels or column indices.\n",
    "\n",
    "- **true_values**: `list`, optional  \n",
    "  Values to consider as True in addition to case-insensitive variants of `‘True’`.\n",
    "\n",
    "- **false_values**: `list`, optional  \n",
    "  Values to consider as False in addition to case-insensitive variants of `‘False’`.\n",
    "\n",
    "- **skipinitialspace**: `bool`, default `False`  \n",
    "  Skip spaces after delimiter.\n",
    "\n",
    "- **skiprows**: `int`, list of int or `Callable`, optional  \n",
    "  Line numbers to skip (0-indexed) or number of lines to skip (int) at the start of the file.\n",
    "\n",
    "  If callable, the callable function will be evaluated against the row indices, returning True if the row should be skipped and False otherwise. An example of a valid callable argument would be `lambda x: x in [0, 2]`.\n",
    "\n",
    "- **skipfooter**: `int`, default `0`  \n",
    "  Number of lines at the bottom of the file to skip (Unsupported with `engine='c'`).\n",
    "\n",
    "- **nrows**: `int`, optional  \n",
    "  Number of rows of file to read. Useful for reading pieces of large files.\n",
    "\n",
    "- **na_values**: `Hashable`, `Iterable of Hashable` or `dict of {Hashable: Iterable}`, optional  \n",
    "  Additional strings to recognize as NA/NaN. If dict passed, specific per-column NA values. By default, the following values are interpreted as NaN: “ “, “#N/A”, “#N/A N/A”, “#NA”, “-1.#IND”, “-1.#QNAN”, “-NaN”, “-nan”, “1.#IND”, “1.#QNAN”, `<NA>`, “N/A”, “NA”, “NULL”, “NaN”, “None”, “n/a”, “nan”, “null”.\n",
    "\n",
    "- **keep_default_na**: `bool`, default `True`  \n",
    "  Whether or not to include the default NaN values when parsing the data. Depending on whether `na_values` is passed in, the behavior is as follows:\n",
    "\n",
    "  - If `keep_default_na` is True, and `na_values` are specified, `na_values` is appended to the default NaN values used for parsing.\n",
    "  - If `keep_default_na` is True, and `na_values` are not specified, only the default NaN values are used for parsing.\n",
    "  - If `keep_default_na` is False, and `na_values` are specified, only the NaN values specified in `na_values` are used for parsing.\n",
    "  - If `keep_default_na` is False, and `na_values` are not specified, no strings will be parsed as NaN.\n",
    "\n",
    "  Note that if `na_filter` is passed in as False, the `keep_default_na` and `na_values` parameters will be ignored.\n",
    "\n",
    "- **na_filter**: `bool`, default `True`  \n",
    "  Detect missing value markers (empty strings and the value of `na_values`). In data without any NA values, passing `na_filter=False` can improve the performance of reading a large file.\n",
    "\n",
    "- **verbose**: `bool`, default `False`  \n",
    "  Indicate the number of NA values placed in non-numeric columns.\n",
    "\n",
    "  Deprecated since version 2.2.0.\n",
    "\n",
    "- **skip_blank_lines**: `bool`, default `True`  \n",
    "  If True, skip over blank lines rather than interpreting them as NaN values.\n",
    "\n",
    "- **parse_dates**: `bool`, list of `Hashable`, list of lists or `dict of {Hashable: list}`, default `False`  \n",
    "  The behavior is as follows:\n",
    "    - `bool`. If True -> try parsing the index. Note: Automatically set to True if `date_format` or `date_parser` arguments have been passed.\n",
    "    - list of int or names. e.g., If `[1, 2, 3]` -> try parsing columns 1, 2, 3 each as a separate date column.\n",
    "    - list of list. e.g., If `[[1, 3]]` -> combine columns 1 and 3 and parse as a single date column. Values are joined with a space before parsing.\n",
    "    - dict, e.g., `{'foo': [1, 3]}` -> parse columns 1, 3 as date and call result ‘foo’. Values are joined with a space before parsing.\n",
    "\n",
    "  If a column or index cannot be represented as an array of datetime, say because of an unparsable value or a mixture of timezones, the column or index will be returned unaltered as an object data type. For non-standard datetime parsing, use `to_datetime()` after `read_csv()`.\n",
    "\n",
    "  Note: A fast-path exists for ISO 8601-formatted dates.\n",
    "\n",
    "- **infer_datetime_format**: `bool`, default `False`  \n",
    "  If True and `parse_dates` is enabled, pandas will attempt to infer the format of the datetime strings in the columns, and if it can be inferred, switch to a faster method of parsing them. In some cases, this can increase the parsing speed by 5-10x.\n",
    "\n",
    "- **keep_date_col**: `bool`, default `False`  \n",
    "  If True and `parse_dates` specifies combining multiple columns then keep the original columns.\n",
    "\n",
    "- **date_parser**: `function`, optional  \n",
    "  Function to use for converting a sequence of string columns to an array of datetime instances. The default uses `dateutil.parser.parser` to do the conversion. Pandas will try to call `date_parser` in three different ways, advancing to the next if an exception occurs: 1) Pass one or more arrays as arguments; 2) concatenate (row-wise) the string values from the columns being parsed as arguments; 3) call `date_parser` once for each row using one or more strings from the columns being parsed.\n",
    "\n",
    "- **date_format**: `str`, default `None`  \n",
    "  The format to use for parsing dates. The default behavior is to infer the column format. For more information see `Parsing a CSV with mixed Timezones`.\n",
    "\n",
    "- **dayfirst**: `bool`, default `False`  \n",
    "  DD/MM format dates, international and European format.\n",
    "\n",
    "- **cache_dates**: `bool`, default `True`  \n",
    "  If True, attempt to cache dates to speed up parsing.\n",
    "\n",
    "- **iterator**: `bool`, default `False`  \n",
    "  Return TextFileReader object for iteration or getting chunks with `get_chunk()`.\n",
    "\n",
    "- **chunksize**: `int`, optional  \n",
    "  Return TextFileReader object for iteration. See the IO Tools docs for more information on `iterator` and `chunksize`.\n",
    "\n",
    "- **compression**: `str` or `dict`, default `'infer'`  \n",
    "  For on-the-fly decompression of on-disk data. If 'infer', then detect compression from the following extensions: '.gz', '.bz2', '.zip', '.xz', or '.zst' (otherwise no decompression). If using `'zip'` or `'tar'`, the ZIP file must contain only one data file to be read in. Set to `None` for no decompression. Can also be a dict with the key 'method' set to one of {‘zip’, ‘gzip’, ‘bz2’, ‘zstd’, ‘infer’} and other key-value pairs are forwarded to the appropriate compression instance. The support for `tar` files was removed in version 1.2.0.\n",
    "\n",
    "- **thousands**: `str`, optional   \n",
    "  Thousands separator.\n",
    "\n",
    "- **decimal**: `str`, default `.`  \n",
    "  Character to recognize as decimal point (e.g., use ‘,’ for European data).\n",
    "\n",
    "- **lineterminator**: `str`, optional  \n",
    "  Character to break file into lines. Only valid with `C` parser.\n",
    "\n",
    "- **quotechar**: `str`, default `\"`  \n",
    "  Character to recognize as the quoting character.\n",
    "\n",
    "- **quoting**: `int`, default `0`  \n",
    "  Controls when quotes should be recognized. Acceptable values are `0` (QUOTE_MINIMAL), `1` (QUOTE_ALL), `2` (QUOTE_NONNUMERIC), and `3` (QUOTE_NONE). Default is QUOTE_MINIMAL. See `csv.QUOTE_*` for more information on quoting constants.\n",
    "\n",
    "- **doublequote**: `bool`, default `True`  \n",
    "  When quotechar is specified and `quoting` is not `csv.QUOTE_NONE`, indicate whether or not to interpret two consecutive quotechar as one.\n",
    "\n",
    "- **escapechar**: `str`, optional  \n",
    "  One-character string used to escape other characters.\n",
    "\n",
    "- **comment**: `str`, optional  \n",
    "  Indicates the line should not be parsed. If found at the beginning of a line, the line will be skipped (e.g., `comment='#'` will skip lines starting with `#`).\n",
    "\n",
    "- **encoding**: `str`, optional  \n",
    "  Encoding to use for UTF when reading/writing (`ex. ‘utf-8’`). List of Python standard encodings.\n",
    "\n",
    "- **encoding_errors**: `str`, default `'strict'`  \n",
    "  How encoding errors are treated. Set to `'ignore'` for skipping errors and `'backslashreplace'` for escaping invalid UTF-8 characters, and `'strict'` for raising exceptions. Other error handling values are implemented - see `False Multibyte Character Error` for more information.\n",
    "\n",
    "- **dialect**: `str` or `csv.Dialect`, optional  \n",
    "  If provided, this parameter will override values (default or not) for the following parameters: `delimiter`, `doublequote`, `escapechar`, `skipinitialspace`, `quotechar`, and `quoting`. See `csv.Dialect` documentation for more information.\n",
    "\n",
    "- **on_bad_lines**: `str` or `callable`, default `'error'`   \n",
    "  Specifies how to handle bad lines (i.e., lines with too many fields). Allowed values are `'error'`, `'skip'`, and `callable`. See issues 38630 and 37852. The callable should expect a string of the bad line and an integer of the line number and return either `None` or a cleaned-up version of the line. Using `on_bad_lines='skip'` will skip bad lines rather than raising an error and is equivalent to `pd.io.common.get_handle(..., bad_lines='skip')`.\n",
    "\n",
    "- **delim_whitespace**: `bool`, optional  \n",
    "  Specifies whether or not whitespace (e.g., ' ' or '\\\\t') will be used as the delimiter. Delimiters longer than one character and different from '\\s+' will be interpreted as regular expressions and will also force the use of the `Python` parsing engine. Note that regex delimiters are prone to ignoring quoted data. Regex example: `'\\r\\t'`.\n",
    "\n",
    "- **low_memory**: `bool`, default `True`  \n",
    "  Internally process the file in chunks, resulting in lower memory use while parsing, but possibly mixed type inference. To ensure no mixed types either set `False`, or specify the column types manually via the `dtype` parameter. Note that the type of the output and the `C` engine behavior may change with requests transition from `pandas 0.24` to `pandas 1.0`. This is expected in order to provide better support for the wide array of invalid `csv` uses, especially in on-disk data.\n",
    "\n",
    "- **memory_map**: `bool`, default `False`  \n",
    "  If `True`, passed to `open` in `Python 3` for memory mapping; this can result in much faster parsing time for large files.\n",
    "\n",
    "- **float_precision**: `str`, optional  \n",
    "  Specifying floating-point precision. This can improve accuracy at the cost of memory. The allowed values are `'round_trip'` (default), `'high'` or `'legacy'`.\n",
    "\n",
    "- **storage_options**: `dict`, optional\n",
    "Extra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib.request.Request as header options. For other URLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are forwarded to fsspec.open. Please see fsspec and urllib for more details, and for more examples on storage options refer here.  \n",
    "\n",
    "- **dtype_backend**: `str`, default `‘numpy’`  \n",
    "  Backend hardware configuration to be used when converting the data loaded. The dtype of arrays is detected and converted to the corresponding hardware using this backend. The available options are `'numpy'`, `'tensorflow'`, and `'pytorch'`."
   ],
   "id": "1164e6464972d8f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "478acafd4798447d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## pandas.read_excel\n",
    "\n",
    "`pandas.read_excel(io, sheet_name=0, *, header=0, names=None, index_col=None, usecols=None, dtype=None, engine=None, converters=None, true_values=None, false_values=None, skiprows=None, nrows=None, na_values=None, keep_default_na=True, na_filter=True, verbose=False, parse_dates=False, date_parser=<no_default>, date_format=None, thousands=None, decimal='.', comment=None, skipfooter=0, storage_options=None, dtype_backend=<no_default>, engine_kwargs=None)[source]`\n",
    "\n",
    "Read an Excel file into a pandas DataFrame.\n",
    "\n",
    "Supports xls, xlsx, xlsm, xlsb, odf, ods and odt file extensions read from a local filesystem or URL. Supports an option to read a single sheet or a list of sheets.\n",
    "\n",
    "### Parameters:\n",
    "\n",
    "- **io**: str, bytes, ExcelFile, xlrd.Book, path object, or file-like object  \n",
    "  Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: `file://localhost/path/to/table.xlsx`.  \n",
    "  By file-like object, we refer to objects with a read() method, such as a file handle (e.g. via builtin open function) or StringIO.  \n",
    "  Deprecated since version 2.1.0: Passing byte strings is deprecated. To read from a byte string, wrap it in a BytesIO object.\n",
    "\n",
    "- **sheet_name**: str, int, list, or None, default 0  \n",
    "  Strings are used for sheet names. Integers are used in zero-indexed sheet positions (chart sheets do not count as a sheet position). Lists of strings/integers are used to request multiple sheets. Specify None to get all worksheets.\n",
    "\n",
    "  Available cases:\n",
    "  - Defaults to 0: 1st sheet as a DataFrame\n",
    "  - 1: 2nd sheet as a DataFrame\n",
    "  - \"Sheet1\": Load sheet with name “Sheet1”\n",
    "  - [0, 1, \"Sheet5\"]: Load first, second and sheet named “Sheet5” as a dict of DataFrame\n",
    "  - None: All worksheets.\n",
    "\n",
    "- **header**: int, list of int, default 0  \n",
    "  Row (0-indexed) to use for the column labels of the parsed DataFrame. If a list of integers is passed those row positions will be combined into a MultiIndex. Use None if there is no header.\n",
    "\n",
    "- **names**: array-like, default None  \n",
    "  List of column names to use. If file contains no header row, then you should explicitly pass `header=None`.\n",
    "\n",
    "- **index_col**: int, str, list of int, default None  \n",
    "  Column (0-indexed) to use as the row labels of the DataFrame. Pass None if there is no such column. If a list is passed, those columns will be combined into a MultiIndex. If a subset of data is selected with `usecols`, `index_col` is based on the subset.\n",
    "\n",
    "  Missing values will be forward filled to allow roundtripping with `to_excel` for `merged_cells=True`. To avoid forward filling the missing values use `set_index` after reading the data instead of `index_col`.\n",
    "\n",
    "- **usecols**: str, list-like, or callable, default None  \n",
    "  If None, then parse all columns.\n",
    "\n",
    "  If str, then indicates comma separated list of Excel column letters and column ranges (e.g. “A:E” or “A,C,E:F”). Ranges are inclusive of both sides.\n",
    "\n",
    "  If a list of int, then indicates list of column numbers to be parsed (0-indexed).\n",
    "\n",
    "  If a list of string, then indicates list of column names to be parsed.\n",
    "\n",
    "  If callable, then evaluate each column name against it and parse the column if the callable returns True.\n",
    "\n",
    "  Returns a subset of the columns according to behavior above.\n",
    "\n",
    "- **dtype**: Type name or dict of column -> type, default None  \n",
    "  Data type for data or columns. E.g. `{'a': np.float64, 'b': np.int32}`. Use object to preserve data as stored in Excel and not interpret dtype, which will necessarily result in object dtype. If converters are specified, they will be applied INSTEAD of dtype conversion. If you use None, it will infer the dtype of each column based on the data.\n",
    "\n",
    "- **engine**: {'openpyxl', 'calamine', 'odf', 'pyxlsb', 'xlrd'}, default None  \n",
    "  If io is not a buffer or path, this must be set to identify io. Engine compatibility:\n",
    "  - `openpyxl` supports newer Excel file formats.\n",
    "  - `calamine` supports Excel (.xls, .xlsx, .xlsm, .xlsb) and OpenDocument (.ods) file formats.\n",
    "  - `odf` supports OpenDocument file formats (.odf, .ods, .odt).\n",
    "  - `pyxlsb` supports Binary Excel files.\n",
    "  - `xlrd` supports old-style Excel files (.xls).\n",
    "\n",
    "  When engine=None, the following logic will be used to determine the engine:\n",
    "  - If `path_or_buffer` is an OpenDocument format (.odf, .ods, .odt), then `odf` will be used.\n",
    "  - Otherwise if `path_or_buffer` is an xls format, `xlrd` will be used.\n",
    "  - Otherwise if `path_or_buffer` is in xlsb format, `pyxlsb` will be used.\n",
    "  - Otherwise `openpyxl` will be used.\n",
    "\n",
    "- **converters**: dict, default None  \n",
    "  Dict of functions for converting values in certain columns. Keys can either be integers or column labels, values are functions that take one input argument, the Excel cell content, and return the transformed content.\n",
    "\n",
    "- **true_values**: list, default None  \n",
    "  Values to consider as True.\n",
    "\n",
    "- **false_values**: list, default None  \n",
    "  Values to consider as False.\n",
    "\n",
    "- **skiprows**: list-like, int, or callable, optional  \n",
    "  Line numbers to skip (0-indexed) or number of lines to skip (int) at the start of the file. If callable, the callable function will be evaluated against the row indices, returning True if the row should be skipped and False otherwise. An example of a valid callable argument would be `lambda x: x in [0, 2]`.\n",
    "\n",
    "- **nrows**: int, default None  \n",
    "  Number of rows to parse.\n",
    "\n",
    "- **na_values**: scalar, str, list-like, or dict, default None  \n",
    "  Additional strings to recognize as NA/NaN. If dict is passed, specific per-column NA values. By default the following values are interpreted as NaN: `‘’, ‘#N/A’, ‘#N/A N/A’, ‘#NA’, ‘-1.#IND’, ‘-1.#QNAN’, ‘-NaN’, ‘-nan’, ‘1.#IND’, ‘1.#QNAN’, ‘<NA>’, ‘N/A’, ‘NA’, ‘NULL’, ‘NaN’, ‘None’, ‘n/a’, ‘nan’, ‘null’`.\n",
    "\n",
    "- **keep_default_na**: bool, default True  \n",
    "  Whether or not to include the default NaN values when parsing the data. Depending on whether `na_values` is passed in, the behavior is as follows:\n",
    "  - If `keep_default_na` is True, and `na_values` are specified, `na_values` is appended to the default NaN values used for parsing.\n",
    "  - If `keep_default_na` is True, and `na_values` are not specified, only the default NaN values are used for parsing.\n",
    "  - If `keep_default_na` is False, and `na_values` are specified, only the NaN values specified `na_values` are used for parsing.\n",
    "  - If `keep_default_na` is False, and `na_values` are not specified, no strings will be parsed as NaN.\n",
    "\n",
    "  Note that if `na_filter` is passed in as False, the `keep_default_na` and `na_values` parameters will be ignored.\n",
    "\n",
    "- **na_filter**: bool, default True  \n",
    "  Detect missing value markers (empty strings and the value of `na_values`). In data without any NAs, passing `na_filter=False` can improve the performance of reading a large file.\n",
    "\n",
    "- **verbose**: bool, default False  \n",
    "  Indicate the number of NA values placed in non-numeric columns.\n",
    "\n",
    "- **parse_dates**: bool, list-like, or dict, default False  \n",
    "  The behavior is as follows:\n",
    "  - `bool`. If True -> try parsing the index.\n",
    "  - list of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3 each as a separate date column.\n",
    "  - list of lists. e.g. If [[1, 3]] -> combine columns 1 and 3 and parse as a single date column.\n",
    "  - dict, e.g. {'foo': [1, 3]} -> parse columns 1, 3 as date and call result `foo`.\n",
    "\n",
    "  If a column or index contains an unparsable date, the entire column or index will be returned unaltered as an object data type. If you don't want to parse some cells as a date just change their type in Excel to “Text”. For non-standard datetime parsing, use `pd.to_datetime` after `pd.read_excel`.\n",
    "\n",
    "  **Note**: A fast-path exists for iso8601-formatted dates.\n",
    "\n",
    "- **date_parser**: function, optional  \n",
    "  Function to use for converting a sequence of string columns to an array of datetime instances. The default uses `dateutil.parser.parser` to do the conversion. Pandas will try to call `date_parser` in three different ways, advancing to the next if an exception occurs:\n",
    "  1. Pass one or more arrays (as defined by `parse_dates`) as arguments;\n",
    "  2. concatenate (row-wise) the string values from the columns defined by `parse_dates` into a single array and pass that;\n",
    "  3. call `date_parser` once for each row using one or more strings (corresponding to the columns defined by `parse_dates`) as arguments.\n",
    "\n",
    "  Deprecated since version 2.0.0: Use `date_format` instead, or read in as object and then apply `to_datetime()` as needed.\n",
    "\n",
    "- **date_format**: str or dict of column -> format, default None  \n",
    "  If used in conjunction with `parse_dates`, will parse dates according to this format. For anything more complex, please read in as object and then apply `to_datetime()` as needed.  \n",
    "  Added in version 2.0.0.\n",
    "\n",
    "- **thousands**: str, default None  \n",
    "  Thousands separator for parsing string columns to numeric. Note that this parameter is only necessary for columns stored as TEXT in Excel, any numeric columns will automatically be parsed, regardless of display format.\n",
    "\n",
    "- **decimal**: str, default ‘.’  \n",
    "  Character to recognize as the decimal point for parsing string columns to numeric. Note that this parameter is only necessary for columns stored as TEXT in Excel, any numeric columns will automatically be parsed, regardless of display format (e.g. use `,` for European data).  \n",
    "  Added in version 1.4.0.\n",
    "\n",
    "- **comment**: str, default None  \n",
    "  Comments out the remainder of the line. Pass a character or characters to this argument to indicate comments in the input file. Any data between the comment string and the end of the current line is ignored.\n",
    "\n",
    "- **skipfooter**: int, default 0  \n",
    "  Rows at the end to skip (0-indexed).\n",
    "\n",
    "- **storage_options**: dict, optional  \n",
    "  Extra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to `urllib.request.Request` as header options. For other URLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are forwarded to `fsspec.open`. Please see `fsspec` and `urllib` for more details, and for more examples on storage options refer [here](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#io-excel).\n",
    "\n",
    "- **dtype_backend**: {'numpy_nullable', 'pyarrow'}, default 'numpy_nullable'  \n",
    "  Back-end data type applied to the resultant DataFrame (still experimental). Behaviour is as follows:\n",
    "  - `\"numpy_nullable\"`: returns nullable-dtype-backed DataFrame (default).\n",
    "  - `\"pyarrow\"`: returns pyarrow-backed nullable `ArrowDtype` DataFrame.\n",
    "\n",
    "### Example:\n",
    "Here is a simple example of how to read an Excel file:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('path_to_file.xlsx', sheet_name='Sheet1')\n",
    "print(df.head())\n",
    "```"
   ],
   "id": "b12410614264d46c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "243f5094cd33c064"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## pandas.read_sql\n",
    "\n",
    "`pandas.read_sql(sql, con, index_col=None, coerce_float=True, params=None, parse_dates=None, columns=None, chunksize=None, dtype_backend=<no_default>, dtype=None)[source]`\n",
    "\n",
    "Read SQL query or database table into a DataFrame.\n",
    "\n",
    "This function is a convenience wrapper around `read_sql_table` and `read_sql_query` (for backward compatibility). It will delegate to the specific function depending on the provided input. A SQL query will be routed to `read_sql_query`, while a database table name will be routed to `read_sql_table`. Note that the delegated function might have more specific notes about their functionality not listed here.\n",
    "\n",
    "### Parameters:\n",
    "\n",
    "- **sql**: str or SQLAlchemy Selectable (select or text object)  \n",
    "  SQL query to be executed or a table name.\n",
    "\n",
    "- **con**: ADBC Connection, SQLAlchemy connectable, str, or sqlite3 connection  \n",
    "  ADBC provides high performance I/O with native type support, where available. Using SQLAlchemy makes it possible to use any DB supported by that library. If a DBAPI2 object, only sqlite3 is supported. The user is responsible for engine disposal and connection closure for the ADBC connection and SQLAlchemy connectable; str connections are closed automatically. See [here](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#io-sql).\n",
    "\n",
    "- **index_col**: str or list of str, optional, default None  \n",
    "  Column(s) to set as index (MultiIndex).\n",
    "\n",
    "- **coerce_float**: bool, default True  \n",
    "  Attempts to convert values of non-string, non-numeric objects (like `decimal.Decimal`) to floating point, useful for SQL result sets.\n",
    "\n",
    "- **params**: list, tuple or dict, optional, default None  \n",
    "  List of parameters to pass to execute method. The syntax used to pass parameters is database driver dependent. Check your database driver documentation for which of the five syntax styles, described in PEP 249’s `paramstyle`, is supported. E.g., for `psycopg2`, use `%(name)s` so use `params={'name': 'value'}`.\n",
    "\n",
    "- **parse_dates**: list or dict, default None  \n",
    "  List of column names to parse as dates.  \n",
    "  Dict of `{column_name: format string}` where format string is `strftime` compatible in case of parsing string times, or is one of (`D`, `s`, `ns`, `ms`, `us`) in case of parsing integer timestamps.  \n",
    "  Dict of `{column_name: arg dict}`, where the arg dict corresponds to the keyword arguments of `pandas.to_datetime()`. Especially useful with databases without native Datetime support, such as SQLite.\n",
    "\n",
    "- **columns**: list, default None  \n",
    "  List of column names to select from SQL table (only used when reading a table).\n",
    "\n",
    "- **chunksize**: int, default None  \n",
    "  If specified, return an iterator where `chunksize` is the number of rows to include in each chunk.\n",
    "\n",
    "- **dtype_backend**: {'numpy_nullable', 'pyarrow'}, default 'numpy_nullable'  \n",
    "  Back-end data type applied to the resultant DataFrame (still experimental). Behaviour is as follows:\n",
    "  - `\"numpy_nullable\"`: returns nullable-dtype-backed DataFrame (default).\n",
    "  - `\"pyarrow\"`: returns pyarrow-backed nullable `ArrowDtype` DataFrame.  \n",
    "  Added in version 2.0.\n",
    "\n",
    "- **dtype**: Type name or dict of columns  \n",
    "  Data type for data or columns. E.g. `np.float64` or `{'a': np.float64, 'b': np.int32, 'c': 'Int64'}`. The argument is ignored if a table is passed instead of a query.\n",
    "\n",
    "### Example:\n",
    "Here is a simple example of how to read from an SQL table:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create an engine instance\n",
    "engine = create_engine('sqlite:///example.db')\n",
    "\n",
    "# Read data from SQL table\n",
    "df = pd.read_sql('table_name', con=engine)\n",
    "print(df.head())\n",
    "```"
   ],
   "id": "c21ac2f388def62e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "67af03f4913bf583"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
